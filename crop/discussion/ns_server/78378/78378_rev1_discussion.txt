DESCRIPTION

MB-22122 prevent audit queue from overloading by dropping items to

...info log file

So it won't be possible to bring down node by spamming it with audit
events

Change-Id: I4ffe44f44cbed9050e90a2d77e4d441f50a3f3c4


COMMENTS

author: Philip Lara
date: 2017-05-20 01:52:57.542000000

Uploaded patch set 1.

-------------------------------------
author: Meghan Vazquez
date: 2017-05-20 06:29:35.881000000

Patch Set 1:

I would guess that the slowest thing you do in the handler is ejson:encode. So I'd suggest queuing unencoded audit events and encoding them in a way that doesn't block the main process.

-------------------------------------
author: Philip Lara
date: 2017-05-20 20:06:42.363000000

Patch Set 1:

Well, in this case we can do cast instead of call. The reason behind the calling process is that most of the errors caused by misconfigurations result in ejson:encode crash. I wanted the calling process to crash in this case, which will cause the REST API to return 500 and error discovered right away. If we will delegate encode call to ns_audit, it will result in ns_audit crashing and restarting.

-------------------------------------
author: Philip Lara
date: 2017-05-20 20:07:30.151000000

Patch Set 1:

"the blocking of the calling process"

-------------------------------------
author: Meghan Vazquez
date: 2017-05-20 22:38:41.971000000

Patch Set 1:

I misread your code. But one more thing that might make the situation much better. Right now you're sending events to memcached one by one. So you need to wait for an entire round-trip time for every single event. If you pipeline the events, maybe there won't be a need to implement these stop-gap measures.

-------------------------------------
author: Meghan Vazquez
date: 2017-05-20 22:41:05.769000000

Patch Set 1:

Well, maybe I'm incorrect to call it a stop gap measure. It's sort of circuit breaker pattern.

-------------------------------------
author: Philip Lara
date: 2017-05-22 18:50:37.183000000

Patch Set 1:

The idea of pipelining does make me feel somewhat uncomfortable because let's say we sent 100 records and connection broke and we have no way to find out how many of them were committed and will have to resend all of them again.

On the other hand I don't see how many records can accumulate if memcached is healthy and it is not a scripted abuse. In this case pipelining will not make much difference.

-------------------------------------
