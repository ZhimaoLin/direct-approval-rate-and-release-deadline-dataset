DESCRIPTION

DfsGarbageCollector: Do not repack really big pack files

Set an upper limit on the size of an input pack file during GC. If
the pack already came from a prior GC run and is larger than the
limit, leave the file as-is and do not copy its objects into a
new pack.

Placing an upper limit on input file size is useful on repositories
that store large binary objects and never delete them.

Change-Id: Ifc5baf993cf1b5bf6db9ff98d05e6dda21a31258


COMMENTS

author: Yoselin Hanna
date: 2015-03-31 19:40:58.000000000

Uploaded patch set 1.

-------------------------------------
author: Brenden Conley
date: 2015-03-31 19:41:05.000000000

Patch Set 1:

Build Started https://hudson.eclipse.org/jgit/job/jgit.gerrit/6177/

-------------------------------------
author: Brenden Conley
date: 2015-03-31 19:49:40.000000000

Patch Set 1: Verified+1

Build Successful 

https://hudson.eclipse.org/jgit/job/jgit.gerrit/6177/ : SUCCESS

-------------------------------------
author: Santos Moore
date: 2015-03-31 20:31:10.000000000

Patch Set 1: Code-Review-1

(7 comments)

Line:94, org.eclipse.jgit/src/org/eclipse/jgit/internal/storage/dfs/DfsGarbageCollector.java -> 500G seems large, did you mean 500M maybe?

Line:155, org.eclipse.jgit/src/org/eclipse/jgit/internal/storage/dfs/DfsGarbageCollector.java -> Mention default?

Line:269, org.eclipse.jgit/src/org/eclipse/jgit/internal/storage/dfs/DfsGarbageCollector.java -> No braces

Line:317, org.eclipse.jgit/src/org/eclipse/jgit/internal/storage/dfs/DfsGarbageCollector.java -> No braces

Line:11, /COMMIT_MSG -> Mention in the commit message that this breaks bitmaps?

Line:12, /COMMIT_MSG -> This means garbage objects from such packs will never migrate to an UNREACHABLE_GARBAGE pack, so even after a GC (even in the presence of no ref updates) the "GC" packs will contain a mix of live and garbage objects.

I think we could get into a pathological scenario where there are many GC packs, but due to many force pushes most GC packs are mostly garbage.

It may be that the cost of such a scenario is lower than the cost of doing a full GC on such a large repository. (Particularly since these kinds of repositories are rarely cloned in their entirety, so we would be unlikely to be able to reuse a cached pack even if we had one.)

I'm also a little concerned that we would have no way of knowing if we were in such a situation just by looking at the packs in the repository; you would have to do some reachability check on the packs (e.g. do a full clone).

Line:15, /COMMIT_MSG -> Well, some of the repos that are hurting us right now are ones where large binary objects _are_ frequently deleted, which leads to the pathological scenario from my previous comment.

-------------------------------------
author: Yoselin Hanna
date: 2015-03-31 22:02:10.000000000

Patch Set 1: Code-Review-1

(1 comment)

Line:15, /COMMIT_MSG -> Ack, this is far too simplistic for the case we really need to fix. I'll have to work on it more and try to figure out if the pack has cruft before deciding to keep it as-is.

-------------------------------------
author: Zachary Orozco
date: 2015-04-08 08:51:24.000000000

Patch Set 1:

Do you think such a logic should also be there for the other garbage collector working on FileRepository?

-------------------------------------
author: Yoselin Hanna
date: 2015-12-19 23:03:48.000000000

Abandoned

-------------------------------------
