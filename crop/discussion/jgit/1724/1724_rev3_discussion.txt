DESCRIPTION

[RFC] Improve reading of large deltas

Some large deltas are strictly forward references, which means they
don't need to seek backwards.  A few use back references, but keep
referencing the same handful of data segments repeatedly, for example
copying an XML element tag name from an earlier location in the base
file.  This latter property happens because delta encoders typically
only examine the first 64 occurrences of any data block.

For these cases try to avoid needing to materialize the entire base by
caching frequently referenced blocks of the base, up to the configured
stream threshold for the window cache.

When there is a reverse seek and the cache isn't large enough to
absorb it, cache the base of that object as a plain file in the new
$GIT_DIR/objects/delta-base-cache directory.  This permits the delta
stream to read from the base using RandomAccessFile, which should have
higher throughput than needing to close, reopen and skip through an
inflater stream.

Similar to the in-memory implementation we cache the delta base and
not the delta itself, as the base is more likely to be requested.

To prevent silent data corruption from sneaking in, the cache file is
segmented into blocks, with each block protected by a CRC 32 checksum.
When creating the cache file, the object's SHA-1 is verified at the
same time that the CRC 32 is computed, so that data corruption doesn't
carry into the cache file.  During reads a whole block is read along
with its CRC 32 code, and the block is validated.  This permits the
reader to detect data corruption without needing to examine the entire
cache file.

This is only an RFC as there is no upper limit on the size of the
delta-base-cache directory.  Files are created, but there is no method
to remove older entries when the cache gets large.

Change-Id: I94de3c8b7e3b9e373c77f7190f1910dab79d9d04
Signed-off-by: Jaycee Hickman xxx@xxx.xxx


COMMENTS

author: Yoselin Hanna
date: 2010-10-12 05:44:12.000000000

Uploaded patch set 3.

-------------------------------------
author: Yoselin Hanna
date: 2010-10-12 05:50:41.000000000

Patch Set 3:

I'm reasonably happy with this now.  We don't always need to create the base cache.  When we do, we should be pretty good at detecting data corruption before it spreads further.

The missing part is cleaning up the delta-base-cache directory automatically, so users don't need to do housekeeping.

-------------------------------------
author: Blake Oneal
date: 2010-10-13 20:20:46.000000000

Patch Set 3: Looks good to me, but someone else must approve

(2 inline comments)

Can we do something with a gc to clean this up after a period of time?

Line:135, org.eclipse.jgit/src/org/eclipse/jgit/storage/file/LargeDeltaBaseInputStream.java -> Worth putting a check with an exception if the block size isn't a power of 2? Probably very JIT friendly.

if(desiredBlockSize & desiredBlockSize-1 != 0 )
  throw new IllegalArgumentException("Invalid block size")

Line:335, org.eclipse.jgit/src/org/eclipse/jgit/storage/file/LargeDeltaBaseInputStream.java -> Note that if basePath is on a different filing system (or drive, in Windows, I guess) then the rename operation may fail. We could try a copy if the rename fails. We could read and write the bits to the basePath (or basePath+"~" or similar)

-------------------------------------
author: Yoselin Hanna
date: 2010-10-14 20:21:22.000000000

Patch Set 3: (2 inline comments)



Line:135, org.eclipse.jgit/src/org/eclipse/jgit/storage/file/LargeDeltaBaseInputStream.java -> No, it isn't worth enforcing that this be a power of 2.  Its just suggested.  I'm not going to bother with this check.

Line:335, org.eclipse.jgit/src/org/eclipse/jgit/storage/file/LargeDeltaBaseInputStream.java -> Its impossible for basePath to be on a different file system than tmp.  Above we construct tmp to be a temporary file in the same directory as the file named by basePath.  No system that I know of permits a single directory to have its file contents spread across different file systems such that a rename within that directory would fail.

Its sufficient here to assume that all files in the same directory are part of the same filesystem, and that we can rename safely from a unique temporary file name in that directory to a stable name within the same directory.  If that fails, its either because the stable name already exists (made at the same time by another thread or process perhaps?) OR the filesystem is utter crap and we can't work with it.  In the former case we'll just retry to read the file made by the other process, and probably succeed at using it.  In the latter case, well, the filesystem is utter crap.  If I can't rename a file in the same directory, I can't provide most of the data safety rules that Git normally provides.  Users should get a better filesystem.

-------------------------------------
author: Yoselin Hanna
date: 2011-05-31 15:45:07.000000000

Patch Set 3: Abandoned

Based on what C Git is doing, I think this will not be necessary. The C Git approach for big files is to avoid delta compression on them, thus avoiding the need to handle large delta streams.

-------------------------------------
