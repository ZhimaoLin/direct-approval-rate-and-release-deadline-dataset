DESCRIPTION

Permit disabling birthday attack checks in PackParser

Reading a repository for millions of missing objects might be very
expensive to perform, especially if the repository is on a network
filesystem or some other costly RPC backend.  A repository owner
might choose to accept some risk in return for better performance,
so allow disabling collision checking when receiving a pack.

Currently there is no way for an end-user to disable this feature.
This is intentional, because it is generally *NOT* a good idea to
skip this check.  Instead this feature is supplied for storage
implementations to bypass the default checking logic, should they
have their own custom routines that is just as effective but can
be handled more efficiently.

Change-Id: I90c801bb40e86412209de0c43e294a28f6a767a5
Signed-off-by: Jaycee Hickman xxx@xxx.xxx


COMMENTS

author: Juliet Cantu
date: 2011-01-22 10:49:09.000000000

Patch Set 1: (1 inline comment)



Line:899, org.eclipse.jgit/src/org/eclipse/jgit/transport/PackParser.java -> Not quite related to this patch per-se, but this check would not work for very large objects since the JVM would be able to load everything into an array.
Second, I think the compressed data would usually also be identical so we could check the compressed data first and only decompress and check the real data if that fails.

-------------------------------------
author: Juliet Cantu
date: 2011-01-22 10:50:36.000000000

Patch Set 1: Looks good to me, but someone else must approve; IP review completed



-------------------------------------
author: Yoselin Hanna
date: 2011-01-22 23:19:44.000000000

Patch Set 1: (1 inline comment)



Line:899, org.eclipse.jgit/src/org/eclipse/jgit/transport/PackParser.java -> Yes, but we already have the incoming data fully loaded into a byte[].  So there are only two reasons the existing data won't also fit into a byte[]:  the JVM heap is full and we get OutOfMemoryError during allocation, that turns into a LargeObjectException and the receive aborts.  The other is the incoming object doesn't match lengths and is way shorter, again we either get LargeObjectException from getCachedBytes or Arrays.equals would fail fast when the array lengths aren't the same.  So I'm not too worried about the object-not-fitting case here.

Actually, PackParser just cannot handle objects that are deltas and don't fit into memory.  It can do large whole objects, but that's a different code path which streams both sides during checking for collisions.

We cannot rely on the compressed data validation to determine if they are the same or not.  Yes, if they had the same exact compressed stream they have the same content, but our local copy might be loose, so its compressed content differs.  Or our local copy might be compressed with a different compression level than what we are receiving, or even just a different version of libz which may have generated a slight difference.  Or our local copy is a delta against something else, so its compressed content is very different.  So I think we would all too often see the compressed validation fail, but then the inflated validation pass.  Which makes the compressed validation somewhat worthless.

This entire "lets check what we already have" rule exists because readers cannot prefer the existing content.  In my DHT storage implementation I'm making it explicit which content copy came before another copy, which makes it simple for a reader to prefer the already existing content.  Thus I don't need to compare the contents, the reader would automatically ignore the new copy and prefer the older one we already had.  Later garbage collection can remove the newer copy.

-------------------------------------
