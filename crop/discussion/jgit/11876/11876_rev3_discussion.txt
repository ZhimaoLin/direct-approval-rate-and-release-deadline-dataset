DESCRIPTION

Split delta search buckets by byte weight

[TODO: Remove debugging printlns.]

Instead of assuming all objects cost the same amount of time to
delta compress, aggregate the byte size of objects in the list
and partition threads with roughly equal total bytes.

Before splitting the list select the N largest paths and assign
each one to its own thread. This allows threads to get through the
worst cases in parallel before attempting smaller paths that are
more likely to be splittable.

By running the largest path buckets first on each thread the likely
slowest part of compression is done early, while progress is still
reporting a low percentage. This gives users a better impression of
how fast the phase will run. On very complex inputs the slow part
is more likely to happen first, making a user realize its time to
go grab lunch, or even run it overnight.

If the worst sections are earlier, memory overruns may show up
earlier, giving the user a chance to correct the configuration and
try again before wasting large amounts of time. It also makes it
less likely the delta compression phase reaches 92% in 30 minutes
and then crawls for 10 hours through the remaining 8%.

Change-Id: I7621c4349b99e40098825c4966b8411079992e5f


COMMENTS

author: Yoselin Hanna
date: 2013-04-15 15:35:27.000000000

Uploaded patch set 3.

-------------------------------------
author: Brenden Conley
date: 2013-04-15 15:35:32.000000000

Patch Set 3:

Build Started https://hudson.eclipse.org/sandbox/job/jgit.gerrit/3381/ 

-------------------------------------
author: Brenden Conley
date: 2013-04-15 15:46:20.000000000

Patch Set 3: Verified

Build Successful 

https://hudson.eclipse.org/sandbox/job/jgit.gerrit/3381/ : SUCCESS

-------------------------------------
