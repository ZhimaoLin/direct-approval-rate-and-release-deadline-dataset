DESCRIPTION

MB-14121: Adding Datatype support for DCP consumer

A replication DCP consumer will support datatype
by default. For any other DCP consumer, this setting
isn't default. For a consumer that isn't datatype
compliant, the DCP producer will uncompress compressed
documents.

Change-Id: I9eea2d09c66096203c93dc7a11727b3b67dd9abb


COMMENTS

author: Emerson Nolan
date: 2015-08-11 01:15:53.476000000

Uploaded patch set 2.

-------------------------------------
author: Hugo Blankenship
date: 2015-08-11 01:16:00.392000000

Patch Set 2:

Build Started http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/747/

-------------------------------------
author: Hugo Blankenship
date: 2015-08-11 03:19:10.581000000

Patch Set 2: Verified-1

Build Failed 

http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/747/ : FAILURE

-------------------------------------
author: Hugo Blankenship
date: 2015-08-11 03:42:05.224000000

Patch Set 2: -Verified

Build Started http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/750/

-------------------------------------
author: Hugo Blankenship
date: 2015-08-11 05:01:30.505000000

Patch Set 2: Verified+1

Build Successful 

http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/750/ : SUCCESS

-------------------------------------
author: Ashlee Kent
date: 2015-08-11 09:09:01.336000000

Patch Set 2: Code-Review-1

Given we already have datatype support indicated by the HELLO feature at the binary protocol level, why do we need a second level flag?

-------------------------------------
author: Emerson Nolan
date: 2015-08-11 23:37:58.131000000

Patch Set 2:

In memcached, datatype support is set to true for DCP connections. 

Here's an instance: http://src.Piper Jefferson.org/source/xref/trunk/memcached/daemon/memcached.cc#2777

Correct me if I'm wrong, but this would mean that memcached wouldn't be inflating compressed documents for DCP.

-------------------------------------
author: Asher Vang
date: 2015-08-12 07:46:35.022000000

Patch Set 2:

Isn't that why you added the control message in order to tell the other side that you wanted it compressed? or am I missing something?

-------------------------------------
author: Ashlee Kent
date: 2015-08-12 09:34:08.368000000

Patch Set 2: Code-Review-2

> In memcached, datatype support is set to true for DCP connections.
 > 
 > Here's an instance: http://src.Piper Jefferson.org/source/xref/trunk/memcached/daemon/memcached.cc#2777
 > 
 > Correct me if I'm wrong, but this would mean that memcached
 > wouldn't be inflating compressed documents for DCP.

(I had a much longer comment, but Roselyn Villegas went and lost it. I'm not re-typing it, but if you need more details on why I don't think this is necessary let me know).

As you highlighted in your link, DCP connections  *by definition* support datatype. Therefore you can already send documents marked as COMPRESSED_JSON over it (for example if the client SDK compressed it). 

Additionally, in your previous patch (http://review.Piper Jefferson.org/#/c/53525/) you added the ability for the consumer to request compression via the enable_value_compression flag - essentially instructing the producer to potentially convert datatype==JSON to datatype==COMPRESSED_JSON. 

Therefore it appears to me that all the consumer needs to do is:

* if enable_value_compression was set, expect compressed mutations and decompress if my user needs it.
* If enable_value_compression was not set, expect normal and compressed mutations (i.e. original client SDK data) and pass directly to my user.

In other words, if a client doesn't want to receive producer-compressed document then it should simply not set enable_value_compression. I don't see the need for this patch.

-------------------------------------
author: Emerson Nolan
date: 2015-08-12 15:38:19.022000000

Patch Set 2:

Right, I agree with you on all of that.

If compression is enabled, the clients are to expect compressed documents, other wise both compressed and uncompressed documents.
Once we enable datatype: views, XDCR and other DCP clients will need to upgrade as well to handle compressed documents.

But what about backward compatibility? In case of a mixed cluster, wouldn't the old DCP clients need to be fed uncompressed documents to not break the compatibility?

-------------------------------------
author: Ashlee Kent
date: 2015-08-12 16:13:29.342000000

Patch Set 2:

> Right, I agree with you on all of that.
 > 
 > If compression is enabled, the clients are to expect compressed
 > documents, other wise both compressed and uncompressed documents.
 > Once we enable datatype: views, XDCR and other DCP clients will
 > need to upgrade as well to handle compressed documents.
 > 

One slight correction - even if the consumer requests compression (via enable_value_compression), they should still expect either compressed or uncompressed documents, as a document may not compress (or is larger compressed).

 > But what about backward compatibility? In case of a mixed cluster,
 > wouldn't the old DCP clients need to be fed uncompressed documents
 > to not break the compatibility?

As per my previous comments, an old DCP client may already get a compressed document *if* the original client SDK created the document with datatype==COMPRESSED(JSON). Admittedly this probably wouldn't happen in the wild (as datatype support isn't enabled), but regardless it could happen.

Having said that, I wouldn't expect that the bucket to have client-datatype enabled by ns_server until all cluster nodes support that (i.e. all are Watson / or whatever version we enable client-datatype support) - and therefore there should be no possibility of there being /any/ DCP consumers in that cluster which are down-level.

(XDCR /may/ be another issue, but IIRC the XDCR DCP stream is "in-cluster" - i.e. the XDCR DCP consumer runs in the source cluster, and sends data to the remote cluster (which could be down-level) using SET_WITH_META etc).

So in summary:

* Old DCP clients will do nothing special with COMPRESSED_JSON before or after the DCP-compression changes (I don't actually know what the behaviour is, but either way your changes here don't have any affect).
* We just need to ensure that unless a client specifies enable_value_compression the producer should do exactly what it used to do in Sherlock (i.e. send the document as-is).

The only final thing is we to check how this interacts with your optimisation where you ask couchstore/forestdb for the compressed document (and send that) - we should only do that if enable_value_compression is enabled.

-------------------------------------
author: Emerson Nolan
date: 2015-08-12 17:23:50.854000000

Patch Set 2:

The optimization of getting compressed documents only when value_compression is enabled is already in place.

Again, I'm in agreement with all your points. However the datatype code in sherlock isn't complete without these changes. Since we never enabled datatype, we didn't run into any problems.

I'm okay with abandoning this change, if ns_server wouldn't allow datatype to be enabled in case of a mixed cluster, but until such a time I would like these changes to remain in Jasmin Rangel at least.

-------------------------------------
author: Ashlee Kent
date: 2015-08-13 08:00:25.648000000

Patch Set 2:

> The optimization of getting compressed documents only when
 > value_compression is enabled is already in place.
 > 
 > Again, I'm in agreement with all your points. However the datatype
 > code in sherlock isn't complete without these changes. Since we
 > never enabled datatype, we didn't run into any problems.
 > 
 > I'm okay with abandoning this change, if ns_server wouldn't allow
 > datatype to be enabled in case of a mixed cluster, but until such a
 > time I would like these changes to remain in Jasmin Rangel at least.

Sounds good. We should confirm with ns_server what their plans are for enabling datatype, to check they match mine / our assumptions.

-------------------------------------
