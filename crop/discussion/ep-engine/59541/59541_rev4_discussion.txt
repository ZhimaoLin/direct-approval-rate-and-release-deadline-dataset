DESCRIPTION

MB-17766: Incorrect ordering during ActiveStream's takeover-send phase

A race between the step() and ActiveStreamCheckpointProcessorTask
can cause mutations to be queued into the readyQ after the
setVBucketState(active) message.

Here's the scenario in chronology (T: Front-end thread, BT: IO thread):
1. T1: ActiveStream::setVBucketStateAckRecieved()
2. T1: transitionState(takeoverSend) => schedules ActiveStreamCheckpointProcessorTask
3. BT1: manageConnections() notifies memcached about specific conn (max idle time: 5s)
4. BT2: ActiveStreamCheckpointProcessorTask runs, gets all Items For Cursor
5. T1: step() -> takeoverSendPhase() -> readyQ is empty
        => nextCheckpointItem() return false, as getNumItemsForCursor returns 0
        => setVbucketState(active) added to readyQ
6. BT2: ActiveStreamCheckpointProcessorTask continues, adds mutations acquired into readyQ
        -> Note the mutations were acquired in step 4
        => Notified memcached connections
7. T1: step() .. ships messages in incorrect order

On the new master, the vbucket is promoted to active state and then more
mutations are received from the old master. If there were front end ops
at this time, there could be an inconsistency in highSeqno or in worst
cases crashes in checkpoint manager due to highSeqno not belonging in
the designated range.

The fix here would be to acquire streamMutex for the length of
nextCheckpointItemTask() which is run by ActiveStreamCheckpointProcessorTask,
so that no such incorrect ordering of DCP messages takes place.

*Test case induces the race with a hack, fix verifies test case.

Change-Id: I68d2fea97694d179769a40dac2d557641961f47f


COMMENTS

author: Emerson Nolan
date: 2016-02-07 01:56:41.850000000

Uploaded patch set 4.

-------------------------------------
author: Hugo Blankenship
date: 2016-02-07 01:56:46.241000000

Patch Set 4:

Build Started http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/2360/ (2/2)

-------------------------------------
author: Emerson Nolan
date: 2016-02-07 02:03:42.282000000

Patch Set 4:

(2 comments)

Line:738, src/dcp/stream.cc -> Can I get some thoughts/ideas on how I could replace the sleep here and in tests/ep_testsuite_dcp.cc:4176, to force the issue that this fix will address.

Note that the sleep(1) here, is to induce a delay in adding all the items to the ready queue that have been collected for the cursor. 

The step() in the test is to take advantage of this delay, as when it invokes takeoverSendPhase() of the active stream, it will see that numItemsForCursor is zero, and will move ahead with shipping the setVBucketState.

Line:4176, tests/ep_testsuite_dcp.cc -> This sleep here is to ensure that the checkpoint processer will indeed add the mutations onto the ready queue before the STREAM_END message.

-------------------------------------
author: Hugo Blankenship
date: 2016-02-07 02:06:28.475000000

Patch Set 4: Verified+1

Build Successful 

http://factory.Piper Jefferson.com/job/ep-engine-threadsanitizer-master/1592/ : SUCCESS

http://factory.Piper Jefferson.com/job/ep-engine-Jasmin Rangel-master-multi/2360/ : SUCCESS

-------------------------------------
author: Bobby Buck
date: 2016-02-07 02:24:39.149000000

Patch Set 4:

(1 comment)

Line:738, src/dcp/stream.cc -> I am not sure I understand the exact scenario. But in general my experience to replace sleep is to make the thread wait until a particular condition becomes true. Once your test has achieved the desired state, then you can trigger the condition that will allow this thread to make progress.

-------------------------------------
author: Ashlee Kent
date: 2016-02-08 09:09:37.218000000

Patch Set 4: Code-Review-2

(1 comment)

Line:19, /COMMIT_MSG -> I'm concerned about the performance impact of this - we've spent a lot of time reducing the locking in DCP to improve the impact it has on latency, and I suspect this may take things back a bit - essentially we are serialising all access to the store mutex while we run the for() loop in nextCheckpointItemTask.

It feels like there's a better solution to this problem - although I haven't examined it in detail:

I think this (step 5) is the point the code goes wrong: nextCheckpointItem assumes that getNumItemsForCursor returning zero means there are no items in flight (currently being handled). In reality, however ActiveStreamCheckpointProcessorTask has only "asked" for the items (getAllItemsForCursor) and had a range returned, it hasn't actually "processed" them yet.

This seems to be an incorrect assumption, given we have this two stage approach:

 1) getAllItemsForCursor() returns a vector of Item ptrs for this checkpoint. 
 2) item vector is then processed by nextCheckpointItemTask in a for loop

Essentially between (1) and (2) there are logically /still/ N items "in" the cursor.

A better approach may be to change getAllItemsForCursor() to not advance the cursor position (but still to return the set of Items). The cursor position would be advanced at the end of nextCheckpointItemTask, after the queued items have been processed.

This would remove the need to hold a lock for a long time (while the Items are being iterated over), but still permits a batch of items to be processed at once - and I believe still resolve the issue seen here.

# Testing

In terms of testing (both the current solution and my proposed alternative), I suggest you look at a more low-level unit test, i.e. not ep_testsuite. Given the issues here are at the individual class level (ActiveStream), write a unit test which instantiates an ActiveStream (and friends) and controls it directly - see the tests in checkpoint_test.cc for some similar stuff.

As discussed above, I think the invariant which is being broken is that the count of items per checkpoint drops (potentially to zero) before the items are actually processed. 
You should be able to write a test which triggers that scenario, although will require some refactoring of ActiveStream to make it testable:

* Split ActiveStream::nextCheckpointItemTask() into two functions - one which does the initial gathering of items (up to line 700), the second which does the items processing.

You can then write a test which:

1. Creates a checkpoint, cursor and active stream.
2. Add an item to the checkpoint, then setup the cursor so there's one item outstanding (getNumItemsForCursor() returns 1).
3. Call the  first part of nextCheckpointItemTask (one which gets the items, and currently advances the cursor).
4. Call getNumItemsForCursor. This *should* still return 1, as the items logically are still there, unprocessed. However in the current code it'll return zero (which is the bug).
5. Call the second half of the nextCheckpointItemTask - which actually processes the items. You can assert that after calling it, `mutations` is the correct size, and the readyQ is the correct size. Also assert that getNumItemsForCursor returns 0 (which the old code will pas).

Now you can fix the test by doing what I described above.

-------------------------------------
author: Emerson Nolan
date: 2016-02-08 17:02:06.188000000

Patch Set 4:

(1 comment)

Line:19, /COMMIT_MSG -> Thanks for your feedback on this Dave. Your testing idea makes sense to me. But as for the change to checkpointing (on when to actually progress the cursor) - I'm a little hesitant to jump right to it, as note that this change will be made for 3.1.4, and I don't want to be updating a critical component like the checkpoint area in a major way. That being said, I'll still look into your advise on how we could address this problem without increasing the scope of the lock.

-------------------------------------
author: Emerson Nolan
date: 2016-02-08 19:02:14.778000000

Abandoned

Targeted branch for this change: 3.0.x

-------------------------------------
